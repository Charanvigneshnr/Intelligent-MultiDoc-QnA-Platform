The characteristics of video streaming services include long duration, highly real-time, large volume of data, high quality of service and strict requirements for synchronization. This results in a huge traffic in the network, as a result of which congestion occurs in the network which in turn result in dropping the video packet and thereby reducing the quality of service experienced by the user. Therefore, there is a need for a sophisticated algorithm that is capable of managing the video streaming mechanism in both the senders’ side and the client’s side to ensure that there would be a good quality of service. Most of the applications either control the buffer to optimize the streaming effect or prevent the loss of video data packets by adjusting the video content quality according to the available bandwidth which is found to be not enough in reducing the network traffic and providing good quality video services.  
Another solution to this problem is using cache to load or stream the video from nearby locations. In computer terms, cache is a high-speed data storage layer which stores the subset of video content, it is transient in nature, so that the future requests for the sane resource or video content are served up faster than it is possible by accessing the video’s primary storage location. It allows to reuse the previously stored data efficiently. This ensures that the user can easily view the data from the cache thereby ensuring that the quality of service increases tremendously.

The characteristics of Generative AI involves the requirement of extremely large datasets (in this case: 70 Billion Tokens) and they are trained for months in very powerful servers working in clusters. This has 4 drawbacks, the data is limited to a certain period of time, processing takes very high computation power, not portable and expensive as well. Therefore, there is a need for more flexible algorithm that can utilize both the pretrained large language model as well as the unseen data which are given by the user. It is done by using a pretrained model which is hosted on a server which is accessed through an API Token. The documents uploaded are converted into embeddings (vector format) and used as an additional database to the already existing pretrained data. Combined together will provide a customized ChatBot which can answers all the questions from the documents and from the default knowledge base as well. As the documents uploaded are limited in size, it can be quickly trained under minutes and it will be ready to answer all questions. One problem which may arise is privacy concerns. Thankfully this proposed solution does not store the documents, it is just held temporarily for the session. Once the website is closed, the documents are automatically discarded. Depending on the prompt the response generation will take it's time, plus this model always gives well defined and elaborate respones by default which is also controllable. Overall this solution is a very helpful assistant in cases like Educational places, Work Environment, Personalized Health Care, and lot.